\documentclass[runningheads]{llncs}

\newtheorem{mytheorem}{Theorem}
\newtheorem{mylemma}[mytheorem]{Lemma}
\newtheorem{myquestion}[mytheorem]{Question}
\newtheorem{myexample}[mytheorem]{Example}
\newtheorem{myproposition}[mytheorem]{Proposition}
\newtheorem{mycorollary}[mytheorem]{Corollary}
\newtheorem{mydefinition}[mytheorem]{Definition}
\newtheorem{myprinciple}[mytheorem]{Principle}

\begin{document}

\title{AGI and the Knight-Darwin Law}

\author{Samuel Allen Alexander\inst{1}\orcidID{0000-0002-7930-110X}}
\authorrunning{S.\ A.\ Alexander}
\institute{The U.S.\ Securities and Exchange Commission
\email{samuelallenalexander@gmail.com}
\url{https://philpeople.org/profiles/samuel-alexander/publications}}

\maketitle

\begin{abstract}
This is a paper about AGIs (agents with Artificial General Intelligence),
and more specifically, about the structure of the populations that
result when parent AGIs create child AGIs. We argue that such populations must
satisfy a certain property which is similar to a property of biological
populations, the Knight-Darwin Law.
The biological Knight-Darwin law states that it is impossible for one organism
to asexually produce another organism, which asexually produces another,
which asexually produces another, and so on forever:
that any sequence of organisms (each one a child of the previous) must contain
a multi-parent organism, or must terminate. We argue that a similar
structural property holds for AGIs creating AGIs under certain assumptions,
and speculate about implications for human-made AGIs.


\keywords{Intelligence Measurement \and Knight-Darwin Law \and Ordinal Notations
\and Intelligence Explosion}
\end{abstract}

\section{Introduction}

One thing that distinguishes AGIs from weaker AIs is that a truly
strong AGI ought to be capable of programming AGIs\footnote{Within this paper,
our approach to AGI is what Goertzel \cite{goertzel2014artificial} describes
as the Universalist Approach:
we consider ``...an idealized case of AGI, similar to
assumptions like the frictionless plane in physics'', with the hope that by
understanding this ``simplified special
case, we can use the understanding we've gained to address more realistic
cases.''}. After all, the first AGIs will,
apparently, need to be programmed by humans. If AGIs are at least as smart as
humans, and if humans can
build AGIs, then AGIs ought to be able to build AGIs.

AGIs programming AGIs is difficult to reason about. To get our hands on something solid,
we have attempted to find structures that abstractly capture the core essence of
AGIs programming AGIs. This led us to discover
what we call the \emph{Intuitive Ordinal Notation System} (introduced in Section
\ref{notationsystemsection}), an ordinal notation system that gets directly at
the heart of AGIs creating AGIs.

We call an AGI \emph{truthful} if the things it knows are all true,
i.e., if it does not know any falsehoods.
In \cite{alexander2019measuring}, we used the Intuitive Ordinal Notation System to argue
that if a truthful AGI $X$ creates (without external help) a truthful AGI $Y$, in such a way
that $X$ knows the truthfulness of $Y$, then $X$ must be more intelligent than $Y$
in a certain formal sense. The argument is based on the key assumption that if $X$
creates $Y$, without external help, then $X$ necessarily knows $Y$'s sourcecode.

Iterating the above argument, suppose $X_1,X_2,\ldots$
are truthful AGIs such that each $X_i$ creates, and knows the truthfulness and
the sourcecode of, $X_{i+1}$. By the previous paragraph, $X_1$ would be more
intelligent than $X_2$, which would be more
intelligent than $X_3$, and so on. If intelligence is well-founded (i.e.,
if it is impossible for a chain of AGIs to keep getting less and less intelligent
without eventually hitting some minimum possible intelligence level),
then this would imply that it is impossible for such
a list $X_1,X_2,\ldots$ to go on forever: it would have to stop after finitely
many elements.

At first glance, the above results might
seem to suggest skepticism regarding the singularity---or regarding
what Hutter \cite{hutter2012} calls \emph{knowledge explosion}, the idea of
AGIs creating better AGIs, which create even better AGIs, and so on.
But there is a loophole. Suppose $X$ and $X'$ are AGIs
who collaborate to create $Y$. Suppose $X$ contributes sourcecode for
part of $Y$, but keeps it secret from $X'$, and suppose $X'$ contributes
sourcecode for another part of $Y$, but keeps it secret from $X$. Then neither
$X$ nor $X'$ knows $Y$'s full sourcecode, and the above-mentioned argument
breaks down.


The Knight-Darwin law \cite{darwin1898knight}, named after Charles Darwin
and Andrew Knight, is the
principal (rephrased in modern language) that there cannot be an infinite
sequence $X_1,X_2,\ldots$ of biological organisms such that each $X_i$ asexually
parents $X_{i+1}$. In other words, if $X_1,X_2,\ldots$ is any infinite list of
organisms such that each $X_i$ is a biological parent of $X_{i+1}$, then some of the
$X_i$ would need to be multi-parent organisms.
The reader will immediately notice a striking parallel between
this principal and the discussion in the previous two paragraph.

In Section \ref{notationsystemsection} we introduce the Intuitive Ordinal Notation
System.

In Section \ref{informalargumentsection} we argue that if a truthful AGI $X$ creates
a truthful AGI $Y$, such that $X$
knows the truthfulness and the sourcecode of $Y$, then $Y$ is less intelligent
than $X$.

In Section \ref{knightdarwinagisection} we adapt the Knight-Darwin Law from biology to AGI
and speculate about what it might mean for AGI, including implications about
the creation of AGI.

In Section \ref{objectionsection} we address some anticipated objections.

In Section \ref{conclusionsection} we summarize and make concluding remarks.


\section{The Intuitive Ordinal Notation System}
\label{notationsystemsection}

We do not know what, exactly, an AGI is, but we want to study AGIs anyway.
Based on our conviction that an AGI should be capable of programming AGIs,
we would like to come up with a more concrete structure, easier to reason
about, which we can use as a proxy to better understand AGIs.

What sort of structure would capture the essence of an AGI's capability
of programming AGI's? As an initial attempt, how about: ``computer
program that prints computer programs''? That seems like the right direction
to go, but as written, there is no constraint on the printed computer programs, so
this initial attempt seems to capture the essence of an
AGI's capability of writing \emph{computer programs}, rather than of writing \emph{AGIs}.

As a second attempt, how
about: ``computer program that prints computer programs that print
computer programs''? This brings us closer, but this second attempt
seems to capture the essence of an AGI's capability of writing \emph{first-attempt programs},
rather than of writing \emph{AGIs}.

As a third attempt, how about:
``computer program that prints computer programs that print computer programs
that print computer programs''? This brings us closer still, but this third
attempt seems to capture the essence of an AGI's capability of writing
\emph{second-attempt programs}, rather than of
writing \emph{AGIs}.

We need to short-circuit the above process. We need to come up with a notion
X which is equivalent to ``computer program that prints members of X''.
We will fill in the blank X with the phrase ``Intuitive Ordinal Notation''.

\begin{mydefinition}
\label{literalnotationdef}
    (See the following examples)
    We define the Intuitive Ordinal Notations to be the smallest set $\mathcal P$
    of computer programs satisfying the following property:
    \begin{enumerate}
        \item
            For every computer program $p$,
            if $\mathcal P$ contains every output that $p$ prints when $p$ is executed,
            then $p\in\mathcal P$.
    \end{enumerate}
\end{mydefinition}

\begin{myexample}
\label{simpleexamples}
(Some simple examples)
    \begin{itemize}
    \item
    Let $P_0$ be ``End'', a program which immediately stops without printing any outputs.
    Vacuously, every output $x$ that gets printed when $P_0$ is executed is in $\mathcal P$
    (because there are no such outputs). So by condition 2 of
    Definition \ref{literalnotationdef}, $P_0$ is a Literal Ordinal Notation.
    \item
    Let $P_1$ be ``Print(``End'')'', a program which prints the output ``End'' and then
    stops. We already saw $\mbox{``End''}\in\mathcal P$, so by condition 2 of
    Definition \ref{literalnotationdef}, $P_1$ is a Literal Ordinal Notation.
    \item
    Let $P_2$ be ``Print(``Print(``End'')'')'', which prints ``Print(``End'')'' and then
    stops. We already saw $\mbox{``Print(``End'')''}\in\mathcal P$, so by condition 2
    of Definition \ref{literalnotationdef}, $P_2$ is a Literal Ordinal Notation.
    \end{itemize}
\end{myexample}

\begin{myexample}
\label{omegaexample}
(A more interesting example)
    Let $P_\omega$ be the program:
    \[
        \mbox{Let X = `End'; While(True) \{ Print(X); X = ``Print(`'' + X + ``')''; \}}
    \]
    When executed, $P_\omega$ prints ``End'', ``Print(``End'')'',
    ``Print(``Print(``End'')'')'', and so on forever. By similar reasoning as
    in Example \ref{simpleexamples}, all of these are Literal Ordinal Notations.
    Therefore, $P_\omega$ is a Literal Ordinal Notation.
\end{myexample}

Because of the recursive way the Literal Ordinal Notations are defined, there is a
natural way to assign computable ordinal numbers to them. This is why we call them
Literal Ordinal Notations--we think of each Literal Ordinal Notation as notating
(that is, as being a name for) the computable ordinal number assigned to it.
How do we assign a computable ordinal to a Literal Ordinal Notation $x$? Well, because
of the recursive nature of the definition, we can assume (by induction) that we've
already assigned computable ordinals to all the outputs which are printed when $x$
is executed (this makes sense because all such outputs are Literal Ordinal Notations,
by definition). So the answer is clear: assign $x$ the first computable ordinal bigger
than all the computable ordinals assigned to the outputs of $x$.

\begin{mydefinition}
    For any Literal Ordinal Notation $x$, we define a computable ordinal $|x|$
    inductively as follows: $|x|$ is the smallest computable ordinal $\alpha$
    such that $\alpha>|y|$ for every output $y$ that gets printed when $x$ is
    executed.
\end{mydefinition}

\begin{myexample}
    \begin{enumerate}
        \item
        Since $P_0$ (from Example \ref{simpleexamples}) has no outputs,
        it follows that $|P_0|=0$, the smallest computable ordinal.
        \item
        Likewise, $|P_1|=1$ and $|P_2|=2$.
        \item
        Likewise, $P_\omega$ (from Example \ref{omegaexample}) has outputs
        notating $0, 1, 2, \ldots$---all the finite natural numbers---and no
        other outputs. It follows that $|P_\omega|=\omega$, the smallest
        infinite ordinal number.
        \item
        Let $P_{\omega+1}$ be the program ``Print($P_\omega$)'',
        where $P_\omega$ is as in Example \ref{omegaexample}.
        It follows that $|P_{\omega+1}|=\omega+1$, the next ordinal after
        $\omega$.
    \end{enumerate}
\end{myexample}

The Literal Ordinal Notation System is similar to Kleene's $\mathcal O$,
an ordinal notation system devised by Stephen Kleene \cite{kleene1938notation}.

\section{An Informal Version of the Impossibility Argument}
\label{informalargumentsection}

Whatever an AGI is, presumably an AGI knows certain mathematical facts
(or could eventually deduce them, if placed in an isolated room for all eternity).
The following definition offers a way to quantify an AGI's intelligence based
solely on the mathematical facts that the AGI knows. In \cite{alexander2019measuring}
we champion this as an appropriate intelligence measure by arguing that it
captures key components of intelligence such as pattern recognition, creativity, and
the ability to abstract and to generalize.
% In the present paper, we'll add the
% following observations to further justify the definition. In some sense, any
% quantifiable measurement

\begin{mydefinition}
\label{maindefinition}
    The \emph{Literal Ordinal Intelligence} of an AGI $X$ is the smallest computable
    ordinal $\alpha$ such that $\alpha>|p|$ for every Literal Ordinal Notation
    $p$ such that $X$ knows (or would eventually figure out) that $p$ is a
    Literal Ordinal Notation.
\end{mydefinition}

Armed with Definition \ref{maindefinition}, we will give an informal version of the
impossibility result from the Introduction.

\begin{mytheorem}
\label{maintheorem}
    Suppose $X$ is a truthful AGI, and $X$ creates a truthful AGI $Y$.
    Assume $X$ knows $Y$'s sourcecode, and that $X$ knows that $Y$ is truthful.
    Then $X$'s Literal Ordinal Intelligence is strictly greater than $Y$'s
    Literal Ordinal Intelligence.
\end{mytheorem}

\begin{proof}
    Suppose $Y$ were isolated for all eternity with just one instruction:
    to spend eternity enumerating the biggest Literal Ordinal Notations $Y$ could
    think of. This would result in some list $L$ of Literal Ordinal Notations
    enumerated by $Y$. Since $Y$ is an AGI, $L$ must be computable, because $L$ can
    be enumerated by a robot (namely by $Y$). Thus, there is some computer program
    $P$ which outputs exactly the list $L$. Since $X$ knows $Y$'s sourcecode,
    and presumably $X$ is capable of elementary reasoning about sourcecodes and
    how to manipulate sourcecodes, it follows that $X$ can infer $P$--that is to
    say, $X$ knows (or would eventually figure out) the statement, ``$P$ outputs
    $L$, the list of things $Y$ would enumerate if $Y$ went off to enumerate
    Literal Ordinal Notations for all eternity''. Since $X$ knows $Y$ is truthful,
    $X$ can deduce that $L$ contains nothing except Literal Ordinal Notations,
    and thus $X$ can deduce that all the outputs of $P$ are Literal Ordinal Notations,
    and so $X$ knows (or would eventually figure out) that $P$ is a Literal Ordinal Notation.
    Clearly $P$ notates an ordinal bigger than the ordinals notated by anything in $L$,
    so since $X$ knows $P$ is a Literal Ordinal Notation, it follows $X$ has a larger
    Literal Ordinal Intelligence than $Y$.
    \qed
\end{proof}

\section{The Knight-Darwin Law}
\label{knightdarwinagisection}

\begin{quote}
``...it is a general law of nature that no organic being self-fertilises itself
for a perpetuity of generations; but that a cross with another individual
is occasionally---perhaps at long intervals of time---indispensable.''
(Charles Darwin)
\end{quote}

In his Origin of Species \cite{originofspecies}, Charles Darwin devotes many
pages to the above-quoted principal, which would later become known as the
Knight-Darwin Law \cite{darwin1898knight}. In \cite{alexander2013} and
\cite{alexander2015alternative}, based on a close reading of Darwin's text, we argue
that the Knight-Darwin Law is accurately translated into modern mathematical
language as follows.

\begin{myprinciple}
(The Knight-Darwin Law)
It is impossible for there to be an infinite sequence
$x_1,x_2,\ldots$ of living organisms such that each $x_i$
is the lone biological parent of $x_{i+1}$. In other words,
if there are ever organisms $x_1,x_2,\ldots$ such that each
$x_i$ is a biological parent of $x_{i+1}$, then there must
be some $i>1$ such that $x_i$ has multiple distinct biological
parents.
\end{myprinciple}

One of the most important properties of the ordinal numbers is the
fact that they are \emph{well-founded}: by which we mean, there is
no infinite sequence $o_1,o_2,\ldots$ of ordinals such that each
$o_i>o_{i+1}$. In Theorem \ref{maintheorem} we showed that if truthful
AGI $X$ creates truthful AGI $Y$, and if $X$ knows the truthfulness
and the sourcecode of $Y$, then $X$ has a higher Literal Ordinal Intelligence
than $Y$. Combining this with the well-foundedness of the ordinals yields
a theorem which is extremely similar to the Knight-Darwin Law.

\begin{mytheorem}
\label{maintheorem2}
(The Knight-Darwin Law for AGIs)
It is impossible for there to be an infinite sequence
$x_1,x_2,\ldots$ of truthful AGIs such that each $x_i$ is the lone creator of $x_{i+1}$.
In other words, if there are
ever truthful AGIs $x_1,x_2,\ldots$ such that each $x_i$ is a creator of $x_{i+1}$,
then there must be some $i>1$ such that multiple distinct creators collaborated to
create $x_i$.
\end{mytheorem}

\begin{proof}
By contradiction. Assume $x_1,x_2,\ldots$ is an infinite sequence of truthful AGIs such
that each $x_i$ is the lone creator of $x_{i+1}$. For each $i$, let $\alpha_i$
be the Literal Ordinal Intelligence of $x_i$. By Theorem \ref{maintheorem},
$\alpha_1>\alpha_2>\cdots$. This contradicts the well-foundedness of the ordinal
numbers.
\qed
\end{proof}

In the remainder of the section, we will speculate about three implications
Theorem \ref{maintheorem2} might have for AGIs and for AGI research.
We stress that the following speculations are not mathematically rigorous.

\subsection{Motivation for Multi-agent Approaches to AGI}

In the Introduction, we argued that, among other things, an AGI ought to be
capable of programming AGIs. If this is true, Theorem \ref{maintheorem2} suggests
that a fundamental aspect of AGI should be the ability to collaborate with other
AGIs in the creation of new AGIs.

One interesting idea along these lines is that there should be no
such thing as a \emph{solipsistic} AGI, in other words, perhaps it is
fundamentally impossible for an AGI to believe that the entire universe
is imagined by itself. For, if an AGI had such a solipsistic belief, it
seems like it would be difficult for this AGI to collaborate with other AGIs
to create child AGIs. And the Knight-Darwin Law for AGIs (Theorem \ref{maintheorem2})
seems to suggest that collaboration is an important ingredient in AGI creation of AGI.

Along more practical lines, Theorem \ref{maintheorem2} suggests it might be worth
prioritizing research into multi-agent systems, for example
\cite{hibbard2011societies}, \cite{lazaridou2018emergence},
\cite{bansal2017emergent}, and similar work.

prioritizing specific approaches to AI that involve multiple agents. For example,
(cite Brendon's papers).

\subsection{Motivation for AGI variety}

The Knight-Darwin Law for biological organisms closely parallels the Knight-Darwin
Law for AGI. Perhaps it might be fruitful to speculate about ways that the former
can tell us things about the latter. The Knight-Darwin Law was important to Charles
Darwin because it was a foundation for a broader thesis that the strength of a
species depends on the inter-breeding of members of that species of a wide variety.
By analogy, we might speculate that if our goal is to create very strong AGIs, then
we ought to focus on creating a wide variety of different kinds of AGIs, so that
those AGIs can collaborate together\footnote{Anticipated
by \cite{dai2011artificial}.} to co-create even stronger AGIs.

On the other hand, if we want to reduce the danger of AGI getting out of control,
perhaps the lesson to take away is to limit AGI variety. At the most extreme end
of the spectrum, if mankind were to limit itself to only creating one single
type of AGI, then Theorem \ref{maintheorem2} would constrain the extent to which
AGIs could reproduce.


\subsection{AGI genetics}

If it is true, as Theorem \ref{maintheorem2} seems to suggest, that AGI collaboration
is a fundamental requirement for an AGI ``population'' to propagate, then it might
be possible to view AGI through a genetic lens\footnote{Anticipated
by \cite{buchanan1988artificial}.}. If AGIs $X$ and $X'$ co-create child AGI $Y$ by
means of $X$ contributing proprietary sourcecode for one submodule, concealed from $X'$,
while $X'$ contributes proprietary sourcecode for another submodule, concealed from $X$,
perhaps there might be some notion of AGI ``genes''. For example, if $X$ runs on operating
system $O$, and $X'$ runs on operating system $O'$, then perhaps their joint offspring
$Y$ should be considered likely to at least exhibit traces of both $O$ and $O'$.
We could even imagine (although this might be a bit of a stretch) dominant and recessive
genes in AGI.


\section{Objections}
\label{objectionsection}

In this section, we address some anticipated objections that the reader might have.

\subsection{What does Definition \ref{maindefinition} have to do with intelligence?}

Whatever intelligence is, it certainly has core components like pattern-matching,
creativity, and the ability to abstract and to generalize.
I claim that these core parts of intelligence are indispensable if one wants to
be competitive at naming large ordinals. If $\alpha$ is an ordinal whose
notation was obtained using certain tricks, then \emph{any} AGI who can use those
tricks should certainly be able to iterate those same tricks. Thus, to advance from
$\alpha$ to a larger ordinal notation which not just \emph{any} $\alpha$-knowing
AGI could obtain, must require the invention of some new technique or tactic, and
this invention of a new technique manifestly requires some amount of creativity,
pattern-matching, etc. This becomes particularly clear if the reader attempts to
go beyond Example \ref{omegaexample} and notate qualitatively larger ordinals;
see the more extensive examples which we give in \cite{alexander2019measuring}.

Now, to reason by way of analogy, suppose we had a ladder which different AGIs
could climb on, and suppose advancing up the ladder requires exercising different
components of intelligence. Since we have so little other idea of how to measure
intelligence, one way to measure, or at least to estimate, intelligence, would be
to measure how high an AGI can climb on said ladder.

Not all ladders are equally good. A ladder would be particularly poor if it had
a top rung, which many AGIs could reach: for then, the ladder would fail to
distinguish between different AGIs who could reach that top rung, even if one
AGI could reach it with ease and another AGI could only reach it with difficulty.
Even if the ladder was infinite and had no top rung, it would still be a poor
ladder if there were many different AGIs who were capable of scaling the whole
ladder (i.e., of ascending however high they like, on demand)\footnote{Hibbard's
intelligence measure based on hierarchies of environments, as described in
\cite{hibbard2011measuring}, is an example of an infinite ladder where many AGIs are
capable of scaling the whole ladder. Those full-ladder-scalers are all the AGIs which
do not ``have finite intelligence'' in Hibbard's words
(defined implicitly, see Hibbard's Proposition 3). It would be straightforward to
modify Hibbard's intelligence measure to make use of a fast-growing hierarchy
\cite{hardy1904theorem} \cite{wainer1987provably}
in order to grow Hibbard's ladder into the transfinite and reduce (or eliminate?)
the set of AGIs capable of scaling the whole ladder. This transformation would make
Hibbard's measurement ordinal-number-valued rather than natural-number-valued
(perhaps Hibbard himself intuited this, since he used the word ``ordinal'' in his abstract
in its non-technical sense as a synonym for ``natural number'').}
A good ladder
should have the property that for every particular AGI, there is some rung which
that AGI cannot reach.

Definition \ref{maindefinition} provides a good ladder. Every AGI will fail to
reach some rung, because no AGI can enumerate all the ordinal notations: it can
be shown (and would be considered a routine exercise to a computability theorist)
that the set of ordinal notations is non-computable. The rungs which an AGI does
manage to reach, we have argued, require the usage of core components of intelligence
to reach.

\subsection{Shouldn't intelligence depend on environmental interaction?}

We do not claim that Definition \ref{maindefinition} is the ``one true measure'' of
intelligence. In fact, maybe there is no such thing: maybe intelligence is inherently
multi-dimensional. Definition \ref{maindefinition} measures one specific type of
intelligence, a type of intelligence based on mathematical knowledge\footnote{Wang has
correctly pointed out \cite{wang2007} that an AGI consists of much more than merely
a set of mathematical facts.}. It could be the
case that some AGI is very good at playing video games, but poor at notating ordinals,
or vice versa. But any truly broad AGI should at least be capable (if properly
instructed) of doing any reasonable well-defined task, including the task of
notating ordinals.
So while Definition \ref{maindefinition} is not the ``one true measure'', it is a
measure of something having to do with an AGI's abilities. It is possible that we
have chosen the word ``intelligence'' poorly and that a different word, such as
``knowledge-level'', would be more appropriate: but that would not qualitatively change
Theorem \ref{maintheorem}, neither would it
invalidate the parallel between the biological Knight-Darwin Law and the
AGI Knight-Darwin Law.

\subsection{Prohibitively expensive simulation}

The reader might object that Theorem \ref{maintheorem} breaks down if $Y$ is prohibitively
expensive for $X$ to simulate or emulate. But actually, Theorem \ref{maintheorem} and its
proof have nothing to do with simulation or emulation. A person can reason about sourcecode
without needing to run that source-code (indeed, there's a whole industry devoted to
exactly this---so-called \emph{static analysis}). In the same way, if $X$ knows the sourcecode
of $Y$, then $X$ can reason about that sourcecode without actually executing a single line
of it.

\subsection{Recursive enumerability of mathematical theorems}

The reader might object that the set of all mathematical theorems is recursively enumerable,
i.e., there exists a Turing machine that lists all mathematical theorems---in which case we
could build an AGI to build all such theorems, and that AGI would have unbeatable intelligence
according to Definition \ref{maindefinition}, and this seems absurd.
But when logicians say that the set of mathematical theorems is recursively enumerable,
what they mean is that the set of mathematical theorems implied by some fixed, recursively
enumerable base set of axioms, is recursively enumerable. For example, the set of all
theorems of Peano arithmetic is recursively enumerable, or the set of all theorems of
ZFC, etc. But G\"odel's first incompleteness theorem demonstrates that for any recursively
enumerable set of axioms, there is always a true mathematical statement not proved by
those axioms. Thus, there is no reason to believe there is some fixed recursively enumerable
axiom-set such that all AGIs universally are limited to only knowing theorems provable
from that axiom-set.

\section{Conclusion}
\label{conclusionsection}

We reviewed, from \cite{alexander2019measuring}, an intelligence measurement notion
for a truthful AGI based on that AGI's mathematical knowledge.
In short: the Literal Ordinal Intelligence of a truthful AGI is the supremum of the
computable ordinals which have notations which the AGI knows to be notations of
computable ordinals. We discussed why we think this counter-intuitive intelligence
measurement notion indeed measures (at least a certain type of) intelligence.

The intelligence measure which we introduced measures intelligence not using real
numbers, but rather using computable ordinal numbers. These have the so-called
\emph{well-ordering property}: there is no infinite, strictly-descending sequence
of computable ordinals. We proved that if a truthful AGI creates (with no external
assistance) a child truthful AGI, knowing the child's truthfulness and sourcecode,
then the parent must necessarily be more intelligent than the child, at least according
to the intelligence notion introduced in this paper. Together with the well-foundedness
of the ordinals, this allowed us to establish a structural property for AGI populations,
which property closely resembles a property of biological populations\footnote{For
some other interesting work exploring AGI connections to biology,
see \cite{strannegaard2018learning}, \cite{yoshida2017homeostatic}.}, the Knight-Darwin
Law. We speculated about implications of this remarkable parallel between biology and
AGI (our speculations here are not intended to be exhaustive, and we expect there
are many more implications besides). Our ultimate hope is that by better understanding
the process by which AGIs must create new AGIs, we can make progress toward the more
ambitious goal: the creation of AGIs by humans.


\bibliographystyle{splncs04}
\bibliography{agikd}

\end{document}