\documentclass[runningheads]{llncs}

\newtheorem{mytheorem}{Theorem}
\newtheorem{mylemma}[mytheorem]{Lemma}
\newtheorem{myquestion}[mytheorem]{Question}
\newtheorem{myexample}[mytheorem]{Example}
\newtheorem{myproposition}[mytheorem]{Proposition}
\newtheorem{mycorollary}[mytheorem]{Corollary}
\newtheorem{mydefinition}[mytheorem]{Definition}
\newtheorem{myprinciple}[mytheorem]{Principle}

\begin{document}

\title{AGI and the Knight-Darwin Law: the necessity of collaboration for
AGI reproduction}
\titlerunning{AGI and the Knight-Darwin Law}

\author{Samuel Allen Alexander\inst{1}\orcidID{0000-0002-7930-110X}}
\authorrunning{S.\ A.\ Alexander}
\institute{The U.S.\ Securities and Exchange Commission
\email{samuelallenalexander@gmail.com}
\url{https://philpeople.org/profiles/samuel-alexander/publications}}

\maketitle

\begin{abstract}
Can an AGI create a more intelligent AGI?
Under certain idealized assumptions, our answer is:
``Not without outside help''.
This is a paper about the mathematical structure of AGI populations
when parent AGIs create child AGIs. We argue that such populations
satisfy a certain biological law.
Motivated by observations of sexual reproduction in seemingly-asexual
species,
the Knight-Darwin Law states that it is impossible for one organism
to asexually produce another organism, which asexually produces another,
and so on forever:
that any sequence of organisms (each one a child of the previous) must contain
a multi-parent organism, or must terminate.
By showing that a certain intelligence measure decreases when a parent AGI
single-handedly creates a child AGI,
we argue that a similar Law holds for AGIs.


\keywords{Intelligence Measurement \and Knight-Darwin Law \and Ordinal Notations
\and Intelligence Explosion}
\end{abstract}

\section{Introduction}

One thing that distinguishes agents with Artificial General Intelligence (AGIs)
from weaker AIs is that an
AGI ought to be capable of programming AGIs\footnote{Within this paper,
our approach to AGI is what Goertzel \cite{goertzel2014artificial} describes
as the Universalist Approach:
we consider ``...an idealized case of AGI, similar to
assumptions like the frictionless plane in physics'', with the hope that by
understanding this ``simplified special
case, we can use the understanding we've gained to address more realistic
cases.''}. After all, the first AGIs will,
apparently, need to be programmed by humans. If AGIs are at least as smart as
humans, and if humans are smart enough to
build AGIs, then AGIs should also be smart enough to do so.

It is difficult to reason about AGIs programming AGIs. To get our hands on something solid,
we have attempted to find structures that abstractly capture the core essence of
AGIs programming AGIs. This led us to discover
what we call the \emph{Intuitive Ordinal Notation System} (presented in Section
\ref{notationsystemsection}), an ordinal notation system that gets directly at
the heart of AGIs creating AGIs.

We call an AGI \emph{truthful} if the things it knows (or could eventually deduce) are true,
i.e., if it does not know any falsehoods.
In \cite{alexander2019measuring}, we used the Intuitive Ordinal Notation System to argue
that if a truthful AGI $X$ creates (without external help) a truthful AGI $Y$, in such a way
that $X$ knows the truthfulness of $Y$, then $X$ must be more intelligent than $Y$
in a certain formal sense. The argument is based on the key assumption that if $X$
creates $Y$, without external help, then $X$ necessarily knows $Y$'s source code.

Iterating the above argument, suppose $X_1,X_2,\ldots$
are truthful AGIs such that each $X_i$ creates, and knows the truthfulness and
the code of, $X_{i+1}$. By the previous paragraph, $X_1$ would be more
intelligent than $X_2$, which would be more
intelligent than $X_3$, and so on.
In Section \ref{informalargumentsection} we will argue that this implies
it is impossible for such
a list $X_1,X_2,\ldots$ to go on forever: it would have to stop after finitely
many elements\footnote{This may initially seem to contradict some abstract mathematical
constructions \cite{kripke2019ungroundedness}
\cite{visser2002semantics} of infinite descending chains of theories. But those
constructions only work for weaker languages, whereas an AGI presumably comprehends
linguistically strong second-order predicates.}.

At first glance, the above results might
seem to suggest skepticism regarding the singularity---or regarding
what Hutter \cite{hutter2012} calls \emph{intelligence explosion}, the idea of
AGIs creating better AGIs, which create even better AGIs, and so on.
But there is a loophole. Suppose $X$ and $X'$ are AGIs
who collaborate to create $Y$. Suppose $X$ contributes code for
part of $Y$, but keeps it secret from $X'$, and suppose $X'$ contributes
code for another part of $Y$, but keeps it secret from $X$. Then neither
$X$ nor $X'$ knows $Y$'s full source code, and yet if $X$ and $X'$ trust
each other, then both $X$ and $X'$ can trust $Y$, so the above-mentioned
argument breaks down.


The Knight-Darwin Law \cite{darwin1898knight}, named after Charles Darwin
and Andrew Knight, is the
principle (rephrased in modern language) that there cannot be an infinite
sequence $X_1,X_2,\ldots$ of biological organisms such that each $X_i$ asexually
parents $X_{i+1}$. In other words, if $X_1,X_2,\ldots$ is any infinite list of
organisms such that each $X_i$ is a biological parent of $X_{i+1}$, then some of the
$X_i$ would need to be multi-parent organisms.
The reader will immediately notice a striking parallel between
this principle and the discussion in the previous two paragraphs.

In Section \ref{notationsystemsection} we present the Intuitive Ordinal Notation
System.

In Section \ref{informalargumentsection} we argue that if a truthful AGI $X$ creates
a truthful AGI $Y$, such that $X$
knows the truthfulness and the code of $Y$, then $Y$ is less intelligent
than $X$. This argument appeared in a fully rigorous form in \cite{alexander2019measuring},
but in this paper we attempt to make it more approachable by informally giving
the key idea behind it.

In Section \ref{knightdarwinagisection} we adapt the Knight-Darwin Law from biology to AGI
and speculate about what it might mean for AGI, including implications about
the creation of AGI.

In Section \ref{objectionsection} we address some anticipated objections.

In Section \ref{conclusionsection} we summarize and make concluding remarks.


\section{The Intuitive Ordinal Notation System}
\label{notationsystemsection}

We do not know what, exactly, an AGI is, but we want to study AGIs anyway.
Based on our conviction that an AGI should be capable of programming AGIs,
we would like to come up with a more concrete structure, easier to reason
about, which we can use to better understand AGIs.

What structure would capture the essence of an AGI's AGI-programming
capability? As an initial attempt, how about: ``computer
program that prints computer programs''? That seems like the right direction
to go, but as written, there is no constraint on the printed computer programs, so
this initial attempt seems to capture the essence of an
AGI's capability of writing \emph{computer programs}, rather than of writing \emph{AGIs}.

As a second attempt, how
about: ``computer program that prints computer programs that print
computer programs''? This brings us closer, but this second attempt
seems to capture the essence of an AGI's capability of writing \emph{first-attempt programs},
rather than of writing \emph{AGIs}.

As a third attempt, how about:
``computer program that prints computer programs that print computer programs
that print computer programs''? This brings us closer still, but this third
attempt seems to capture the essence of an AGI's capability of writing
\emph{second-attempt programs}, rather than of
writing \emph{AGIs}.

We need to short-circuit the above process. We need to come up with a notion
X which is equivalent to ``computer program that prints members of X''.

\begin{mydefinition}
\label{literalnotationdef}
    (See the following examples)
    We define the Intuitive Ordinal Notations to be the smallest set $\mathcal P$
    of computer programs satisfying the following property:
    \begin{itemize}
        \item
            For every computer program $p$,
            if $\mathcal P$ contains every output that $p$ prints when $p$ is executed,
            then $p\in\mathcal P$.
    \end{itemize}
\end{mydefinition}

\begin{myexample}
\label{simpleexamples}
(Some simple examples)
    \begin{enumerate}
    \item
    Let $P_0$ be ``End'', a program which immediately stops without printing any outputs.
    Vacuously, every output printed when $P_0$ is executed is in $\mathcal P$
    (there are no such outputs). So $P_0$ is an Intuitive Ordinal Notation.
    \item
    Let $P_1$ be ``Print(`End')'', a program which prints ``End'' and then
    stops. By (1), every output printed by $P_1$ is an Intuitive Ordinal Notation,
    therefore, so is $P_1$.
    \item
    Let $P_2$ be ``Print(`Print(`End')')'', which prints ``Print(`End')'' and then
    stops. By (2), every output printed by $P_2$ is an Intuitive Ordinal Notation,
    therefore, so is $P_2$.
    \end{enumerate}
\end{myexample}

\begin{myexample}
\label{omegaexample}
(A more interesting example)
    Let $P_\omega$ be the program:
    \[
        \mbox{\normalfont Let X = `End';
        While(True) \{ Print(X); X = ``Print(`'' + X + ``')''; \}}
    \]
    When executed, $P_\omega$ prints ``End'', ``Print(`End')'',
    ``Print(`Print(`End')')'', and so on forever. As
    in Example \ref{simpleexamples}, all of these are Intuitive Ordinal Notations.
    Therefore, $P_\omega$ is an Intuitive Ordinal Notation.
\end{myexample}

Examples \ref{simpleexamples} and \ref{omegaexample} are reminiscent
of Franz's approach of ``head[ing] for general algorithms at low complexity levels
and fill[ing] the task cup from the bottom up'' \cite{franz2015toward}.
For a much larger collection of examples, see \cite{github}.

\begin{mydefinition}
    For any Intuitive Ordinal Notation $x$, we define a computable ordinal $|x|$
    inductively as follows: $|x|$ is the smallest computable ordinal $\alpha$
    such that $\alpha>|y|$ for every output $y$ that gets printed when $x$ is
    executed.
\end{mydefinition}

\begin{myexample}
    \begin{itemize}
        \item
        Since $P_0$ (from Example \ref{simpleexamples}) has no outputs,
        it follows that $|P_0|=0$, the smallest computable ordinal.
        \item
        Likewise, $|P_1|=1$ and $|P_2|=2$.
        \item
        Likewise, $P_\omega$ (from Example \ref{omegaexample}) has outputs
        notating $0, 1, 2, \ldots$---all the finite natural numbers---and no
        other outputs. It follows that $|P_\omega|=\omega$, the smallest
        infinite ordinal number.
        \item
        Let $P_{\omega+1}$ be the program ``Print($P_\omega$)'',
        where $P_\omega$ is as in Example \ref{omegaexample}.
        It follows that $|P_{\omega+1}|=\omega+1$, the next ordinal after
        $\omega$.
    \end{itemize}
\end{myexample}

The Intuitive Ordinal Notation System is a more intuitive simplification of
an ordinal notation system known as Kleene's $\mathcal O$.

\section{Intuitive Ordinal Intelligence}
\label{informalargumentsection}

Whatever an AGI is, presumably an AGI knows certain mathematical facts
(or could eventually deduce them).
The following is a notion of an AGI's intelligence based
solely on said facts. In \cite{alexander2019measuring}
we argue that this notion captures key components of intelligence such as
pattern recognition, creativity, and the ability to abstract and to
generalize. We will offer additional justification in
Section \ref{objectionsection}.

\begin{mydefinition}
\label{maindefinition}
    The \emph{Intuitive Ordinal Intelligence} of a truthful AGI $X$ is the smallest computable
    ordinal $\alpha$ such that $\alpha>|p|$ for every Intuitive Ordinal Notation
    $p$ such that $X$ knows (or could eventually deduce) that $p$ is an
    Intuitive Ordinal Notation.
\end{mydefinition}

The following theorem provides a relationship\footnote{Formalizing a relationship implied
in an offhand way by Chaitin, who suggests ordinal computation as a mathematical challenge
intended to spur evolution, ``and the larger the ordinal,
the fitter the organism'' \cite{chaitin}.} between Intuitive Ordinal Intelligence
and AGI creation of AGI.

\begin{mytheorem}
\label{maintheorem}
    Suppose $X$ is a truthful AGI, and $X$ creates a truthful AGI $Y$,
    in such a way that $X$ knows $Y$'s code and truthfulness. Then
    $X$ has higher Intuitive Ordinal Intelligence than $Y$.
\end{mytheorem}

\begin{proof}
    Suppose $Y$ were commanded to
    spend eternity enumerating the biggest Intuitive Ordinal Notations $Y$ could
    think of. This would result in some list $L$ of Intuitive Ordinal Notations
    enumerated by $Y$. Since $Y$ is an AGI, $L$ must be computable, because $L$ can
    be enumerated by a robot (namely by $Y$). Thus, there is some computer program
    $P$ whose outputs are exactly $L$. Since $X$ knows $Y$'s code,
    and presumably $X$ is capable of elementary reasoning about code,
    it follows that $X$ can infer $P$---that is,
    $X$ knows (or could eventually deduce) the statement: ``$P$ outputs
    $L$, the list of things $Y$ would print if $Y$ were commanded to spend eternity
    trying to enumerate large Intuitive Ordinal Notations''.
    Since $X$ knows $Y$ is truthful,
    $X$ can deduce that $L$ contains nothing except Intuitive Ordinal Notations,
    thus $X$ can deduce that $P$'s outputs are Intuitive Ordinal Notations,
    and so $X$ knows (or could deduce) that $P$ is an Intuitive Ordinal Notation.
    So $X$'s Intuitive Ordinal Intelligence is $>|P|$. By construction, $|P|$ is
    the least ordinal bigger than $|Q|$ for all $Q$ printed by $L$, in other words,
    $|P|$ is the Intuitive Ordinal Intelligence of $Y$.
    \qed
\end{proof}

Theorem \ref{maintheorem} is mainly intended for the situation where parent $X$ creates
independent child $Y$, but can also be applied in case $X$ self-modifies,
viewing the original $X$ as being replaced by the new self-modified
$Y$ (assuming $X$ has prior
knowledge of the code and truthfulness of the modified result).

\section{The Knight-Darwin Law}
\label{knightdarwinagisection}

\begin{quote}
``...it is a general law of nature that no organic being self-fertilises itself
for a perpetuity of generations; but that a cross with another individual
is occasionally---perhaps at long intervals of time---indispensable.''
(Charles Darwin)
\end{quote}

In his Origin of Species, Darwin devotes many
pages to the above-quoted principle, later called the
Knight-Darwin Law \cite{darwin1898knight}. In \cite{alexander2013}
we translate
the Knight-Darwin Law into mathematical language.

\begin{myprinciple}
(The Knight-Darwin Law)
There cannot be an infinite sequence
$x_1,x_2,\ldots$ of organisms such that each $x_i$
is the lone biological parent of $x_{i+1}$.
If each $x_i$ is a parent of $x_{i+1}$, then some $x_{i+1}$
must have multiple parents.
\end{myprinciple}

Of course Darwin was aware of seemingly asexual plant species, but
a key motivation for the above law was the observation that even
seemingly asexual plant species occasionally reproduce sexually.
For example, a plant in which pollen is ordinarily isolated, might
release said pollen into the air if a storm damages the part of the
plant that would otherwise shield said pollen. Even prokaryotes can
be considered to occasionally have multiple parents if lateral gene
transfer is taken into account.

A key fact about the ordinal numbers is they are
\emph{well-founded}: there is
no infinite sequence $o_1,o_2,\ldots$ of ordinals such that each
$o_i>o_{i+1}$. In Theorem \ref{maintheorem} we showed that if truthful
AGI $X$ creates truthful AGI $Y$, in such a way as to know the truthfulness
and code of $Y$, then $X$ has a higher Intuitive Ordinal Intelligence
than $Y$. Combining this with the well-foundedness of the ordinals yields
a theorem extremely similar to the Knight-Darwin Law.

\begin{mytheorem}
\label{maintheorem2}
(The Knight-Darwin Law for AGIs)
There cannot be an infinite sequence
$x_1,x_2,\ldots$ of truthful AGIs such that each $x_i$
creates $x_{i+1}$ in such a way as to know $x_{i+1}$'s truthfulness and code.
If each $x_i$ creates $x_{i+1}$ (intending $x_{i+1}$ to be truthful), then
some $x_{i+1}$ must be co-created by multiple creators (assuming that creation by
a lone creator implies the lone creator would know $x_{i+1}$'s code).
\end{mytheorem}

\begin{proof}
By Theorem \ref{maintheorem}, the Intuitive Ordinal Intelligence of $x_1,x_2,\ldots$
would be an infinite strictly-descending sequence of ordinal numbers, which would
violate the well-foundedness of the ordinals.
\qed
\end{proof}

Note that it is perfectly consistent with
Theorem \ref{maintheorem} that $Y$ might operate faster
than $X$, thereby performing better in realtime environments (as in \cite{gavane}).
Theorems \ref{maintheorem} and \ref{maintheorem2} are profound because
they suggest that descendants might appear more intelligent
initially, yet, without outside help, they will eventually degenerate.
This parallels the \emph{hydra game} of
Kirby and Paris \cite{kirby1982accessible}, where a hydra
seems to become stronger as the player hacks off its heads, yet
inevitably dies if the player keeps hacking.

In the remainder of this section, we will non-rigorously speculate about three implications
Theorem \ref{maintheorem2} might have for AGIs and for AGI research.


\subsection{Motivation for Multi-agent Approaches to AGI}

In the Introduction, we argued that an AGI ought to be
capable of programming AGIs. If so, Theorem \ref{maintheorem2} suggests
that a fundamental aspect of AGI should be the ability to collaborate with other
AGIs in the creation of new AGIs\footnote{It is particularly interesting that our
intelligence measure naturally motivates multi-agent considerations in spite of
the fact that, a priori, Definition \ref{maindefinition} measures intelligence
with an almost reckless disregard for the surrounding collective of other agents
(contrast \cite{hernandez2011more}) or even for the outside world.}.

One interesting idea along these lines is that there should be no
such thing as a \emph{solipsistic} AGI, or at least, solipsistic AGIs would be
limited in their ability to reproduce.
For, if an AGI were solipsistic, it
seems like it would be difficult for this AGI to collaborate with other AGIs
to create child AGIs.
To quote Hern{\'a}ndez-Orallo et al: ``The appearance of multi-agent systems is a sign that
the future of machine intelligence will not be found in monolithic systems
solving tasks without other agents to compete or collaborate with''
\cite{hernandez2011more}.

Along more practical lines, Theorem \ref{maintheorem2} suggests it may be worth
prioritizing research into multi-agent systems, for example
\cite{castelfranchi1998modelling}, \cite{hernandez2011more},
\cite{hibbard2011societies}, \cite{lazaridou2018emergence},
\cite{thorisson2004constructionist}, \cite{potyka2016group},
\cite{kolonin2018reputation},
and similar work.

\subsection{Motivation for AGI Variety}

The Knight-Darwin Law was important to
Darwin because it was a foundation for a broader thesis that the survival of a
species depends on the inter-breeding of a variety of members of that species.
By analogy, we might speculate that if our goal is to create robust AGIs, then
we ought to focus on creating a wide variety of different kinds of AGIs, so that
those AGIs can collaborate together to co-create even stronger AGIs.

On the other hand, if we want to reduce the danger of AGI getting out of control,
perhaps the lesson to take away is to limit AGI variety. At the most extreme end
of the spectrum, if humankind were to limit itself to only creating one single
AGI\footnote{Or to perfectly isolating
different AGIs away from
one another---see \cite{yampolskiy2012leakproofing}.}, then
Theorem \ref{maintheorem2} would constrain the extent to which
that AGI could reproduce.


\subsection{AGI Genetics}

If, as Theorem \ref{maintheorem2} seems to suggest, AGI collaboration
is a fundamental requirement for an AGI ``population'' to propagate, then it might
someday be possible to view AGI through a genetic lens\footnote{Anticipated
by \cite{buchanan1988artificial}.}. If AGIs $X$ and $X'$ co-create child AGI $Y$ by
means of $X$ contributing proprietary code for one submodule, concealed from $X'$,
while $X'$ contributes proprietary code for another submodule, concealed from $X$,
perhaps there might be some notion of AGI ``genes''. For example, if $X$ runs operating
system $O$, and $X'$ runs operating system $O'$, perhaps their joint offspring
will somehow exhibit traces of both $O$ and $O'$.


\section{Discussion}
\label{objectionsection}

In this section, we discuss some anticipated objections.

\subsection{What does Definition \ref{maindefinition} have to do with intelligence?
Shouldn't intelligence depend on environmental interaction?}

We do not claim that Definition \ref{maindefinition} is the ``one true measure'' of
intelligence. Maybe there is no such thing: maybe intelligence is inherently
multi-dimensional. Definition \ref{maindefinition} measures a type of
intelligence based on mathematical knowledge\footnote{Wang has
correctly pointed out \cite{wang2007} that an AGI consists of much more than merely
a knowledge-set of mathematical facts.}. An AGI could be good at video games
but poor at ordinals. But the broad AGIs we are talking about in this paper
should be capable (if properly
instructed) of attempting any reasonable well-defined task, including that of
notating ordinals. So Definition \ref{maindefinition} does
measure one aspect of an AGI's abilities. It is possible that
a different word, such as
``knowledge-level'', would be more appropriate: but
that would not qualitatively change
the Knight-Darwin Law implications.

Whatever intelligence is, it has core components like pattern-matching,
creativity, and the ability to abstract and to generalize.
I claim that these core parts of intelligence are indispensable if one wants to
competitively name large ordinals. If $\alpha$ is an ordinal whose
notation was obtained using certain techniques, then \emph{any} AGI who used those
techniques to notate $\alpha$ should also be able to iterate those same techniques.
Thus, to advance from
$\alpha$ to a larger ordinal which not just \emph{any} $\alpha$-knowing
AGI could obtain, must require\footnote{To quote Potapov et al: ``No combination of
non-universal (weak) cognitive functions will result in universal
(strong) intelligence'' \cite{potapov2012cognitive}.}
the creative invention of some new technique or tactic, and
this invention of a new technique requires some amount of creativity,
pattern-matching, etc. This becomes clear if the reader attempts to
go beyond Example \ref{omegaexample} and notate qualitatively larger ordinals;
see the more extensive examples in \cite{github}.

We will say more.
For analogy's sake, imagine a ladder which different AGIs
can climb, and suppose advancing up the ladder requires exercising different
components of intelligence. Since we have so little other idea how to measure
intelligence, one way to measure (or at least estimate) it would be
to measure how high an AGI can climb on said ladder.

Not all ladders are equally good. A ladder would be particularly poor if it had
a top rung which many AGIs could reach: for then, the ladder would fail to
distinguish between different AGIs who could reach that top rung, even if one
AGI reaches it with ease and another AGI reaches it with difficulty.
Even if the ladder was infinite and had no top rung, it would still be suboptimal
if there were many different AGIs capable of scaling the whole
ladder (i.e., of ascending however high they like, on demand)\footnote{Hibbard's
intelligence measure
\cite{hibbard2011measuring} is an example of an infinite ladder
which is nevertheless short enough that many AGIs can
scale the whole ladder. Those whole-ladder-scalers are the AGIs which
do not ``have finite intelligence'' in Hibbard's words
(defined implicitly, see Hibbard's Proposition 3). It should be possible to
use a \emph{fast-growing hierarchy}
\cite{fairtlough1998hierarchies} \cite{weiermann2002slow}
to extend Hibbard's ladder into the transfinite and reduce (or eliminate?)
the set of AGIs capable of scaling the whole ladder. This transformation would make
Hibbard's measurement ordinal-number-valued rather than natural-number-valued
(perhaps Hibbard himself intuited this, since his abstract uses the
word ``ordinal''
in its everyday sense as a synonym for ``natural number'').}.
A good ladder
should have the property that for every particular AGI, there is some rung which
that AGI cannot reach.

Definition \ref{maindefinition} provides a good ladder.
The rungs which an AGI does
manage to reach, we have argued, require the usage of core components of intelligence
to reach.
And no particular AGI can scale
the whole ladder,
because no AGI can enumerate all the Intuitive Ordinal Notations: it can
be shown
that the set of Intuitive Ordinal Notations is not computably enumerable\footnote{Thus,
this ladder avoids a common problem that arises when
trying to measure machine intelligence using IQ tests, namely, that for any IQ test,
an algorithm can be designed specifically with the aim of performing well on that
IQ test, despite being otherwise unintelligent \cite{besold2015can}.}.
Neither is our ladder overly-high, because for any particular
ordinal notation, an AGI could presumably be programmed with knowledge of that notation
hard-coded.

\subsection{Can't an AGI just print a copy of itself?}

\begin{quote}
    She wrote me a letter, and she wrote it so kind:
    she put down in writing what was in her mind.
    (Bob Dylan)
\end{quote}

If an AGI knows its own code, then there's nothing stopping that
AGI from printing a copy of itself.
However, if it does so, then it necessarily cannot know the truthfulness of that
copy, lest it would know the truthfulness of itself.
G\"odel's incompleteness theorems imply that a suitably idealized AGI cannot know
its own code
and its own truthfulness (see \cite{alexander2014machine}).

\subsection{Prohibitively expensive simulation}

The reader might object that Theorem \ref{maintheorem} breaks down if $Y$ is prohibitively
expensive for $X$ to simulate or emulate. But Theorem \ref{maintheorem} and its
proof have nothing to do with simulation or emulation. A person can reason about code
without running it. In the same way, if $X$ knows the code
of $Y$, then $X$ can reason about that code without executing a single line
of it.

\subsection{Computable enumerability of mathematical theorems}

``Isn't the set of all mathematical theorems computably enumerable?
Coundn't an AGI just enumerate them all by brute force, and wouldn't that AGI have
unbeatable intelligence according to Definition \ref{maindefinition}?'' The answer is no.
The set of mathematical theorems is only computably enumerable when a computably
enumerable base set of axioms is fixed in the background, in which case,
the theorems provable \emph{from those axioms} are computably enumerable.
For example, the
theorems of Peano arithmetic are computably enumerable.
By G\"odel's incompleteness theorem, for any computably
enumerable true set of axioms capable of arithmetic, there is always a
true mathematical statement not provable by
those axioms. In fact, G\"odel constructively tells us how to obtain such a
statement, and an AGI could easily follow G\"odel's recipe.
%Thus, it is unlikely that there is some fixed computably enumerable
%set of theorems which all AGIs universally are limited to.

\section{Conclusion}
\label{conclusionsection}

%We reviewed, from \cite{alexander2019measuring}, an intelligence measurement notion
%for a truthful AGI based on that AGI's mathematical knowledge.
%In short:
The Intuitive Ordinal Intelligence of a truthful AGI is defined to be the supremum of the
computable ordinals which have notations the AGI knows to be notations of
computable ordinals. We discussed why we think this counter-intuitive intelligence
measurement notion measures (a type of) intelligence.
We proved that if a truthful AGI creates
a child truthful AGI, in such a way as to know the child's truthfulness and code,
then the parent must be more intelligent than the child according
to our intelligence measure. This allowed us to establish a structural
property for AGI populations,
closely resembling a property of biological populations, the Knight-Darwin
Law. We speculated about implications of this parallel between biology and
AGI.
%(our speculations here are not intended to be exhaustive, and we expect there
%are many more implications besides).
We hope that by better understanding
the process by which AGIs must create new AGIs, we can make progress toward
understanding methods of AGI-creation by humans.

\section*{Acknowledgments}

We acknowledge Jordi Bieger, Thomas Forster, Jos{\'e} Hern{\'a}ndez-Orallo,
Bill Hibbard, Mike Steel,
and Albert Visser for comments and feedback.

\bibliographystyle{splncs04}
\bibliography{agikd}

\end{document}